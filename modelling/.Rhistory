lrn <- makeLearner("classif.xgboost", predict.type="prob")
lrn$par.vals = list(
nrounds = 100,
verbose = F,
objective = "binary:logistic"
# to restrict memory and  cpu usage set nthreads = 2
# for multiclass use objective = "multi:softmax"
)
# Wrap our learner so it will randomly downsample the majority class
lrn <- makeUndersampleWrapper(lrn)
# Define hyper parameters, read this https://goo.gl/CMQxha and XGBoost's docs
ps = makeParamSet(
makeNumericParam("eta", lower=0.01, upper=0.3),
makeIntegerParam("max_depth", lower=2, upper=6),
makeIntegerParam("min_child_weight", lower=1, upper=5),
makeNumericParam("colsample_bytree", lower=.5, upper=1),
makeNumericParam("subsample", lower=.5, upper=1),
makeNumericParam("usw.rate", lower=.5, upper=1)
)
# Define random search
ctrl <- makeTuneControlRandom(maxit=random_search_iter, tune.threshold=F)
# Define performane metrics - use at least 2, otherwise get_results won't work
pr10 <- perf_make_pr_measure(recall_thrs, "pr10")
m2 <- auc
m3 <- setAggregation(pr10, test.sd)
m4 <- setAggregation(auc, test.sd)
# It's always the first in the list that's used to rank hyperparams in tuning
m_all <- list(pr10, m2, m3, m4)
# Define outer and inner resampling strategies
outer <- makeResampleDesc("CV", iters=3, stratify=T, predict = "both")
inner <- makeResampleDesc("CV", iters=3, stratify=T)
if (matching){
outer$stratify <- FALSE
inner$stratify <- FALSE
}
# Define wrapped learner: this is mlR's way of doing nested CV on a learner
lrn_wrap <- makeTuneWrapper(lrn, resampling=inner, par.set=ps, control=ctrl,
show.info=F, measures=m_all)
# ------------------------------------------------------------------------------
# Training model with nested CV and save results
# ------------------------------------------------------------------------------
parallelStartSocket(detectCores(), level="mlr.tuneParams")
res <- resample(lrn_wrap, dataset, resampling=outer, models=T, weights=iw,
extract=getTuneResult, show.info=F, measures=m_all)
parallelStop()
readr::write_rds(res, file.path(output_folder, "all_results.rds"))
# ------------------------------------------------------------------------------
# Save results summary, tried parameter combinations and predictions
# ------------------------------------------------------------------------------
extra <- list("Matching"=as.character(matching),
"NumSamples"=dataset$task.desc$size,
"NumFeatures"=sum(dataset$task.desc$n.feat),
"ElapsedTime(secs)"=res$runtime,
"RandomSeed"=random_seed,
"Recall"=recall_thrs,
"IterationsPerFold"=random_search_iter)
results <- results_nested_results(res, grid_ps=ps, extra=extra, detailed=T,
write_csv=T, output_folder=output_folder,
output_csv="results.csv")
# Save all hyperparameters with their results
opt_paths <- results_opt_paths(res)
readr::write_csv(opt_paths, file.path(output_folder, "opt_paths.csv"))
# Get predictions for all samples, and for only outer fold ones
all_preds <- as.data.frame(res$pred)
outer_test_preds <-  results_outer_predictions(res, ids=ids$ID)
#
for(f in 1:3) {
mypred<-as_tibble(outer_test_preds) %>%
filter(iter == f)
res_iter_pred<-res$pred
res_iter_pred$data<- as.data.frame(mypred)
perf_plot_pr_curve(res_iter_pred)
}
library(ggplot2)
# ggplot(subset(outer_test_preds,ID %in% c("P1" , "P3"))) +
#   geom_line(aes(Value1, Value2, group=ID, colour=ID))
#
# g <- ggplot(df, aes(x))
# g <- g + geom_line(aes(y=y1), colour="red")
# g <- g + geom_line(aes(y=y2), colour="green")
# ------------------------------------------------------------------------------
# Plot PR curve and save binned version as .csv file
# ------------------------------------------------------------------------------
# Note this PR curve is from 3 different models
perf_plot_pr_curve(res$pred)
# Find out at which threshold we maximise a given perf metric - PR10 here
## Norman-comment: Just wondering why we optimize three models in parallel!
out<-tuneThreshold(pred=res$pred, measure=pr10)
pr <- perf_binned_perf_curve(res$pred, x_metric="rec", y_metric="prec",
bin_num=20, agg_func = mean)
readr::write_csv(pr$curve, file.path(output_folder, "binned_pr.csv"))
# ------------------------------------------------------------------------------
# Plot any performance metric for each model as a function of threshold
# ------------------------------------------------------------------------------
# Define performance metrics we want to plot, ppv=precision, tpr=recall
## Norman-question: the three variables below have not been defined??
perf_to_plot <- list(fpr, tpr, ppv)
# Generate the data for the plots, do aggregate=T if you want the mean
thr_perf <- generateThreshVsPerfData(res$pred, perf_to_plot, aggregate=F)
plotThreshVsPerf(thr_perf)
# Get models from outer folds
outer_models <- results_models(res)
# ------------------------------------------------------------------------------
# Get and plot variable importances
# ------------------------------------------------------------------------------
results_vi_table(outer_models[[1]], dataset)
plotting_vi(res, aggregate=F)
# Get detailed splits table
results_xgb_splits(outer_models[[1]], dataset)
# ------------------------------------------------------------------------------
# Fit model on all data with average of best params of outer models
# ------------------------------------------------------------------------------
best_mean_params <- results_best_mean_param(results, return_int = T)
lrn_outer <- setHyperPars(lrn, par.vals=best_mean_params)
lrn_outer_trained <- train(lrn_outer, dataset)
lrn_outer_model <-getLearnerModel(lrn_outer_trained, more.unwrap=T)
# ------------------------------------------------------------------------------
# Assess performance of average model
# ------------------------------------------------------------------------------
results_vi_table(lrn_outer_model, dataset)
pred_outer <- predict(lrn_outer_trained, dataset)
perf_plot_pr_curve(pred_outer)
results_vi_table(lrn_outer_model, dataset)
best_mean_params <- results_best_mean_param(results, return_int = T)
lrn_outer <- setHyperPars(lrn, par.vals=best_mean_params)
lrn_outer_trained <- train(lrn_outer, dataset)
lrn_outer_model <-getLearnerModel(lrn_outer_trained, more.unwrap=T)
results_vi_table(lrn_outer_model, dataset)
pred_outer <- predict(lrn_outer_trained, dataset)
perf_plot_pr_curve(pred_outer)
results_vi_table(lrn_outer_model, dataset)
.rs.restartR()
##----eval=FALSE----------------------------------------------------------
# ------------------------------------------------------------------------------
#
#                         Nested CV with XGBoost
#
# ------------------------------------------------------------------------------
library(mlr)
library(parallel)
library(parallelMap)
library(ggplot2)
library(palabmod)
# ------------------------------------------------------------------------------
# Define main varaibles, setup analysis
# ------------------------------------------------------------------------------
# Matching or no matching
matching = TRUE
# Load dataset and var_config
if (matching){
data("breast_cancer_matched")
df = breast_cancer_matched
}else{
data("breast_cancer")
df = breast_cancer
}
data("var_config")
# Get matching and variables
if (matching){
matches <- as.factor(df$match)
}
ids <- utils_get_ids(df, var_config)
df <- utils_get_variables(df, var_config)
# Setup dataset - for imputing missing values have a look at the vignette
target = "Class"
if(matching){
dataset <- makeClassifTask(id="BC", data=df, target=target, positive=1,
blocking=matches)
}else{
dataset <- makeClassifTask(id="BC", data=df, target=target, positive=1)
}
dataset
utils_get_class_freqs(dataset)
# Important variables that will make it to the result file
random_seed <- 123
recall_thrs <- 10
random_search_iter <- 50L
set.seed(random_seed, "L'Ecuyer")
# Define output folder and create it - if it doesn't exist
output_folder = "xgboost"
utils_create_output_folder(output_folder)
# ------------------------------------------------------------------------------
# Setup modelling
# ------------------------------------------------------------------------------
# Define weights as the inverse class frequency
target_vector = getTaskTargets(dataset)
target_tab = as.numeric(table(target_vector))
iw = 1/target_tab[target_vector]
# Define XGboost learner
lrn <- makeLearner("classif.xgboost", predict.type="prob")
lrn$par.vals = list(
nrounds = 100,
verbose = F,
objective = "binary:logistic"
# to restrict memory and  cpu usage set nthreads = 2
# for multiclass use objective = "multi:softmax"
)
# Wrap our learner so it will randomly downsample the majority class
lrn <- makeUndersampleWrapper(lrn)
# Define hyper parameters, read this https://goo.gl/CMQxha and XGBoost's docs
ps = makeParamSet(
makeNumericParam("eta", lower=0.01, upper=0.3),
makeIntegerParam("max_depth", lower=2, upper=6),
makeIntegerParam("min_child_weight", lower=1, upper=5),
makeNumericParam("colsample_bytree", lower=.5, upper=1),
makeNumericParam("subsample", lower=.5, upper=1),
makeNumericParam("usw.rate", lower=.5, upper=1)
)
# Define random search
ctrl <- makeTuneControlRandom(maxit=random_search_iter, tune.threshold=F)
# Define performane metrics - use at least 2, otherwise get_results won't work
pr10 <- perf_make_pr_measure(recall_thrs, "pr10")
m2 <- auc
m3 <- setAggregation(pr10, test.sd)
m4 <- setAggregation(auc, test.sd)
# It's always the first in the list that's used to rank hyperparams in tuning
m_all <- list(pr10, m2, m3, m4)
# Define outer and inner resampling strategies
outer <- makeResampleDesc("CV", iters=3, stratify=T, predict = "both")
inner <- makeResampleDesc("CV", iters=3, stratify=T)
if (matching){
outer$stratify <- FALSE
inner$stratify <- FALSE
}
# Define wrapped learner: this is mlR's way of doing nested CV on a learner
lrn_wrap <- makeTuneWrapper(lrn, resampling=inner, par.set=ps, control=ctrl,
show.info=F, measures=m_all)
# ------------------------------------------------------------------------------
# Training model with nested CV and save results
# ------------------------------------------------------------------------------
parallelStartSocket(detectCores(), level="mlr.tuneParams")
res <- resample(lrn_wrap, dataset, resampling=outer, models=T, weights=iw,
extract=getTuneResult, show.info=F, measures=m_all)
parallelStop()
readr::write_rds(res, file.path(output_folder, "all_results.rds"))
# ------------------------------------------------------------------------------
# Save results summary, tried parameter combinations and predictions
# ------------------------------------------------------------------------------
extra <- list("Matching"=as.character(matching),
"NumSamples"=dataset$task.desc$size,
"NumFeatures"=sum(dataset$task.desc$n.feat),
"ElapsedTime(secs)"=res$runtime,
"RandomSeed"=random_seed,
"Recall"=recall_thrs,
"IterationsPerFold"=random_search_iter)
results <- results_nested_results(res, grid_ps=ps, extra=extra, detailed=T,
write_csv=T, output_folder=output_folder,
output_csv="results.csv")
# Save all hyperparameters with their results
opt_paths <- results_opt_paths(res)
readr::write_csv(opt_paths, file.path(output_folder, "opt_paths.csv"))
# Get predictions for all samples, and for only outer fold ones
all_preds <- as.data.frame(res$pred)
outer_test_preds <-  results_outer_predictions(res, ids=ids$ID)
#
for(f in 1:3) {
mypred<-as_tibble(outer_test_preds) %>%
filter(iter == f)
res_iter_pred<-res$pred
res_iter_pred$data<- as.data.frame(mypred)
perf_plot_pr_curve(res_iter_pred)
}
library(ggplot2)
# ggplot(subset(outer_test_preds,ID %in% c("P1" , "P3"))) +
#   geom_line(aes(Value1, Value2, group=ID, colour=ID))
#
# g <- ggplot(df, aes(x))
# g <- g + geom_line(aes(y=y1), colour="red")
# g <- g + geom_line(aes(y=y2), colour="green")
# ------------------------------------------------------------------------------
# Plot PR curve and save binned version as .csv file
# ------------------------------------------------------------------------------
# Note this PR curve is from 3 different models
perf_plot_pr_curve(res$pred)
# Find out at which threshold we maximise a given perf metric - PR10 here
## Norman-comment: Just wondering why we optimize three models in parallel!
out<-tuneThreshold(pred=res$pred, measure=pr10)
pr <- perf_binned_perf_curve(res$pred, x_metric="rec", y_metric="prec",
bin_num=20, agg_func = mean)
readr::write_csv(pr$curve, file.path(output_folder, "binned_pr.csv"))
# ------------------------------------------------------------------------------
# Plot any performance metric for each model as a function of threshold
# ------------------------------------------------------------------------------
# Define performance metrics we want to plot, ppv=precision, tpr=recall
## Norman-question: the three variables below have not been defined??
perf_to_plot <- list(fpr, tpr, ppv)
# Generate the data for the plots, do aggregate=T if you want the mean
thr_perf <- generateThreshVsPerfData(res$pred, perf_to_plot, aggregate=F)
plotThreshVsPerf(thr_perf)
# Get models from outer folds
outer_models <- results_models(res)
results_vi_table(outer_models[[1]], dataset)
debug(results_vi_table)
results_vi_table(outer_models[[1]], dataset)
target
data
?palabmod:::utils_data_without_target
results_vi_table(outer_models[[1]], dataset)
undebug(results_vi_table)
results_vi_table(outer_models[[1]], dataset)
feature_names <- getTaskFeatureNames(dataset)
feature_names
? getTaskFeatureNames
par_dep_data <- generatePartialDependenceData(res$models[[1]], dataset,
feature_names, fun=median)
? generatePartialDependenceData
plotPartialDependence(par_dep_data)
library(Ecdat)
install.packages("Ecdat")
library(xgboost)
library(Ecdat)
data(Icecream)
install.packages("Ecfun")
library(xgboost)
library(Ecdat)
data(Icecream)
Icecream
bst <- xgboost(data = train.data, label = Icecream$cons, max.depth = 3, eta = 1, nthread = 2, nround = 2, objective = "reg:linear")
train.data <- data.matrix(Icecream[,-1])
bst <- xgboost(data = train.data, label = Icecream$cons, max.depth = 3, eta = 1, nthread = 2, nround = 2, objective = "reg:linear")
xgb.plot.tree(feature_names = names((Icecream[,-1])), model = bst)
install.packages("DiagrammeR")
xgb.plot.tree(feature_names = names((Icecream[,-1])), model = bst)
library(DiagrammeR)
library(XML)
install.packages("XML")
library(XML)
xgb.plot.tree(feature_names = names((Icecream[,-1])), model = bst)
results_vi_table(lrn_outer_model, dataset)
pred_outer <- predict(lrn_outer_trained, dataset)
perf_plot_pr_curve(pred_outer)
vignette(package="palabmod")
browseVignettes("palabmod")
library(mlr)
cars <- datasets::mtcars
View(cars)
cars$Lemon <- rep(c(1,0,1,0), 8)
cars$Lemon
?makeLearner
?makeResampleDesc
rdesc <- makeResampleDesc(method = "CV", iters = 3)
?resample
rdesc <- makeResampleDesc(method = "CV", iters = 3, stratify = TRUE)
res <- resample(learner = lrn_xgb, task = dataset, resampling = rdesc)
lrn_xgb <- makeLearner(cl = "classif.xgboost", predict.type = "prob")
dataset <- makeClassifTask(id = "Lemon_XGBoost", data = cars, target = "Lemon",
positive = 1)
lrn_xgb <- makeLearner(cl = "classif.xgboost", predict.type = "prob")
rdesc <- makeResampleDesc(method = "CV", iters = 3, stratify = TRUE)
res <- resample(learner = lrn_xgb, task = dataset, resampling = rdesc)
dataset <- makeClassifTask(id = "Lemon_XGBoost", data = cars, target = "Lemon",
positive = 1)
cars_num <- as.data.frame(sapply(cars, function(x) { as.numeric(as.character(x)) } ))
dataset <- makeClassifTask(id = "Lemon_XGBoost", data = cars, target = "Lemon",
positive = 1)
dataset <- makeClassifTask(id = "Lemon_XGBoost", data = cars_num, target = "Lemon",
positive = 1)
dataset <- makeClassifTask(id = "Lemon_XGBoost", data = cars_num, target = "Lemon",
positive = 1)
sapply(cars_num, class)
dataset <- makeClassifTask(id = "Lemon_XGBoost", data = cars_num, target = "Lemon",
positive = 1)
dataset <- makeClassifTask(id = "Lemon_XGBoost", data = cars_num, target = "Lemon",
positive = 1)
View(cars_num)
rm(list = ls())
cars_num <- as.data.frame(sapply(cars, function(x) { as.numeric(as.character(x)) } ))
sapply(cars_num, class)
cars <- datasets::mtcars
cars_num <- as.data.frame(sapply(cars, function(x) { as.numeric(as.character(x)) } ))
sapply(cars_num, class)
cars$Lemon <- rep(c(1,0,1,0), 8)
cars_num <- as.data.frame(sapply(cars, function(x) { as.numeric(as.character(x)) } ))
cars_num$Lemon <- as.factor(cars_num$Lemon)
cars_num$Lemon
dataset <- makeClassifTask(id = "Lemon_XGBoost", data = cars_num, target = "Lemon",
positive = 1)
lrn_xgb <- makeLearner(cl = "classif.xgboost", predict.type = "prob")
res <- resample(learner = lrn_xgb, task = dataset, resampling = rdesc)
rdesc <- makeResampleDesc(method = "CV", iters = 3)
res <- resample(learner = lrn_xgb, task = dataset, resampling = rdesc)
library(palabmod)
pr_curve <- perf_binned_perf_curve(pred = res$pred)
pr_curve$curve
?train
xgb_model <- train(learner = lrn_xgb,
task = dataset)
xgb_model$task.desc
xgb_model$features
pred <- predict(object = xgb_model, newdata = cars_num)
pred$predict.type
pred$data
pred$threshold
lrn_xgb <- makeLearner(cl = "classif.xgboost", predict.type = "prob")
exit
bye
quit
setwd("C:/Users/npoh/Git/projects/bi/modelling")
knitr::opts_chunk$set(echo = TRUE)
setwd("C:/Users/npoh/Git/projects/bi/modelling")
library(palab)
setwd("C:/Users/npoh/Git/projects/bi/modelling")
library(palab)
library(palabmod)
browseVignettes("palab") #this views the vignette in your browser
all_packages <- ls(getNamespace("palab"))
all_packages
all_packages <- ls(getNamespace("palab"))
all_packages
list.files("../data/norman_subset")
list.files("../data/norman_subset")
files <- list.files("../data/norman_subset")
files <- list.files("../data/norman_subset")
files
data_dir = "../data/norman_subset"
files <- list.files(data_dir)
files
files]
files[0]
files(0)
data_dir = "../data/norman_subset"
files <- list.files(data_dir)
files[1]
data_dir = "../data/norman_subset"
files <- list.files(data_dir)
files
data_dir = "../data/norman_subset"
files <- list.files(data_dir)
files
var_config_generator(input_csv = paste0(data_dir, files[1]),
prefix = "var_config.csv",
output_dir = ".")
data_dir = "../data/norman_subset/"
files <- list.files(data_dir)
files
var_config_generator(input_csv = paste0(data_dir, files[1]),
prefix = "var_config.csv",
output_dir = ".")
var_config_generator(input_csv = paste0(data_dir, files[1]),
prefix = "bi",
output_dir = ".")
var_config_generator(input_csv = paste0(data_dir, files[1]),
prefix = "bi_",
output_dir = ".")
?var_config_generator
var_config_generator(input_csv = paste0(data_dir, files[1]),
prefix = "bi_",
output_dir = ".")
transformed_mtcars <- read_transform(
input_csv = paste0(data_dir, files[1]),
var_config_csv = "bi_var_config.csv",
#missing_values = "-99, -999",
read_key_as_double = FALSE
)
df <- read_transform(
input_csv = paste0(data_dir, files[1]),
var_config_csv = "bi_var_config.csv",
#missing_values = "-99, -999",
read_key_as_double = TRUE
)
View(df)
df
View(df$data)
df$report
univar_results <- univariate_stats(input = df,
var_config = "bi_var_config.csv",
vargt0 = FALSE)
univar_results <- univariate_stats(input = df$data,
var_config = "bi_var_config.csv",
vargt0 = FALSE)
univar_results
univar_results
univar_results$cat
View(univar_results$cat)
univar_results$cat$`Missing, prop`
library(ggplot2)
p<-ggplot(data=univar_results$cat, aes(x=Variable, y=`Missing, prop`)) +
geom_bar(stat="identity")
p<-ggplot(data=univar_results$cat, aes(x=Variable, y=`Missing, prop`)) +
geom_bar(stat="identity")
p<-ggplot(data=univar_results$cat, aes(x=Variable, y=`Missing, prop`)) +
geom_bar(stat="identity")
p
p<-ggplot(data=univar_results$cat, aes(x=Variable, y=`Missing, prop`)) +
geom_bar(stat="identity")
p + coord_flip()
univar_results$numerical
univar_results$numerical
p<-ggplot(data=univar_results$numerical, aes(x=Variable, y=`prop Missing`)) +
geom_bar(stat="identity")
p + coord_flip()
p<-ggplot(data=univar_results$numerical, aes(x=Variable, y=`Non-missing, prop`)) +
geom_bar(stat="identity")
p + coord_flip()
p<-ggplot(data=univar_results$numerical, aes(x=Variable, y=`missing, prop`)) +
geom_bar(stat="identity")
p + coord_flip()
p<-ggplot(data=univar_results$numerical, aes(x=Variable, y=`Missing, prop`)) +
geom_bar(stat="identity")
p + coord_flip()
p<-ggplot(data=univar_results$numerical, aes(x=Variable, y=`Missing, prop`)) +
geom_bar(stat="identity")
p + coord_flip()
univar_results$numerical$Variable
files
df_neg
df_neg <- read_transform(
input_csv = paste0(data_dir, files[2]),
var_config_csv = "bi_var_config.csv",
#missing_values = "-99, -999",
read_key_as_double = TRUE
)
