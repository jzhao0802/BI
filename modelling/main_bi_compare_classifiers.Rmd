---
title: "BI-IPF comparing features"
author: "Norman Poh"
date: "7 September 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Set up

```{r}
run_on_server <- TRUE
load_data_with_config <- TRUE

if (run_on_server) {
  setwd("K:/Norman/BI_IPF2017/modelling") 
} else {
  setwd("C:/Users/npoh/Documents/Git/projects/bi/modelling")
}

library(palab)
library(palabmod)
library(ggplot2)
library(tidyverse)
library(stringr)
library(lubridate)
library(mlr)
library(tictoc)
library(PRROC)

```

## Start with repeatability in mind
```{r results = "hide"}
random_seed <- 123
set.seed(random_seed, "L'Ecuyer")
```

## Functions


```{r}
# divide_into_train_test_sets: (idlist, train_prop = .75 ) -> two idlist

divide_into_train_test_sets <- function(id_pos, train_prop = .75 ) {
  #INPUT:
  #id_list    : a patient IDs list that is unique
  #train_prop : training proportion between 0 and 1
  #OUTPUT
  #a list with two lists of patient IDs, corresponding to the training and the test set
  
  test_prop <- 1 - train_prop
  
  mylist <- round( c(train_prop, test_prop) * length(id_pos) )
  
  trainlist <- 1:mylist[1]
  testlist <- (mylist[1]+1):length(id_pos)
  
  order <- sample( length(id_pos) )
  
  idlist <-  vector('list',2)
  idlist[[1]] <- id_pos[order[trainlist]]
  idlist[[2]] <- id_pos[order[testlist]]
  
  return(idlist)
}

# logit
logit <- function(x) {
  log(x) - log( 1-x + .Machine$double.eps)
}

```

## load the features
```{r}
tic()
features <- readRDS('features.rds')
toc()
tic()
feature3 <- readRDS('feature3.rds')
toc()
```

## Load the idlist
```{r}
if ( file.exists("idlist.rds")) {
  idlist <- readRDS("idlist.rds")
} else {
  id_pos <- features[[2]]$patient_id
  idlist <- divide_into_train_test_sets(id_pos, train_prop = .75)
  saveRDS(idlist, "idlist.rds")
}
```
## We shall now combine the features
```{r}
# the training set
data_ <- vector('list', 2)

tic()
t <- 1
data_[[t]] <- rbind( subset( features[[2]], matched_patient_id %in% idlist[[t]]) , #pos
                     subset( features[[1]], matched_patient_id %in% idlist[[t]]) ) #neg 200
                    #subset( features[[3]], matched_patient_id %in% idlist[[t]]) ) #neg 653
toc()

# the test set
tic()
t <- 2
# data_[[t]] <- rbind(subset( features[[2]], matched_patient_id %in% idlist[[t]]) , #pos
#                     subset( features[[1]], matched_patient_id %in% idlist[[t]]) ) #neg 200
#                     #subset( features[[3]], matched_patient_id %in% idlist[[t]]) ) #neg 653
data_[[t]] <- rbind(subset( features[[2]], matched_patient_id %in% idlist[[t]]) , #pos
                    subset( features[[1]], matched_patient_id %in% idlist[[t]]) , #neg 200
                    feature3 ) #neg 653
toc()

# check that all samples are used
res_ <- rbind(dim(features[[1]]), dim(features[[2]]),dim(feature3) )
sum(res_[,1])

res_ <- rbind( dim(data_[[2]]), dim(data_[[1]]) )
sum(res_[,1])

nrow(feature3)/653
```
## Clear the memory
```{r}
rm(features, feature3)

```

## xgboost without the DIFF features
XGBoost with the default parameters

```{r model}
#checking number of samples
table(data_[[1]]$label)
table(data_[[2]]$label)

# check dimension
dim(data_[[1]] %>% select(-patient_id, -matched_patient_id, -ends_with("_DIFF")))
dim(data_[[1]])

train_mlr <- makeClassifTask(data=data_[[1]] %>% select(-patient_id, -matched_patient_id, -ends_with("_DIFF")), target="label", positive=1)
test_mlr <- makeClassifTask(data=data_[[2]] %>% select(-patient_id, -matched_patient_id, -ends_with("_DIFF")), target="label", positive=1)

xgb_lrn <- makeLearner(cl = "classif.xgboost", predict.type = "prob")
model <- train(learner = xgb_lrn, task = train_mlr)

# Apply model to data
tic()
pred <- predict(object = model, task = test_mlr)
toc()
#head(pred$data)
#tail(pred$data)

# Check the distribution
res_ <- tibble ( logit = logit(pred$data$prob.1), label = data_[[2]]$label )

ggplot(res_, aes(res_$logit, col=label, fill=label)) + 
    geom_density(alpha=.5) 

# Plot PR curve
pr1 <- pr.curve(scores.class0 = pred$data$prob.1, 
                weights.class0 = (as.numeric(pred$data$truth) - 1), 
                curve = TRUE)
plot(pr1)
write_csv(as.data.frame(pr1$curve), 'xgboost_full.csv')

```
## xgboost with the DIFF features

and with the default parameters

```{r model}

# check dimension
dim(data_[[1]] %>% select(-patient_id, -matched_patient_id))
dim(data_[[1]])

train_mlr <- makeClassifTask(data=data_[[1]] %>% select(-patient_id, -matched_patient_id), target="label", positive=1)
test_mlr <- makeClassifTask(data=data_[[2]] %>% select(-patient_id, -matched_patient_id), target="label", positive=1)

xgb_lrn <- makeLearner(cl = "classif.xgboost", predict.type = "prob")
model_DIFF <- train(learner = xgb_lrn, task = train_mlr)

# Apply model to data
tic()
pred_DIFF <- predict(object = model_DIFF, task = test_mlr)
toc()
head(pred$data)
tail(pred$data)

# Check the distribution
res_ <- tibble ( logit = logit(pred_DIFF$data$prob.1), label = data_[[2]]$label )

ggplot(res_, aes(res_$logit, col=label, fill=label)) + 
    geom_density(alpha=.5) 

# Plot PR curve
pr1 <- pr.curve(scores.class0 = pred_DIFF$data$prob.1, 
                weights.class0 = (as.numeric(pred$data$truth) - 1), 
                curve = TRUE)
plot(pr1)
write_csv(as.data.frame(pr1$curve), 'xgboost.csv')

```

##

```{r}

data__ <- as.data.frame( sapply(data_[[1]], as.double) )
test_ <- as.data.frame( sapply(data__, mean) )

colnames_ <- colnames(data_[[1]])
selected_vars <- colnames_[! is.na(test_$`sapply(data__, mean)`)]


train_lr <- makeClassifTask(data=data_[[1]] %>% select(one_of(selected_vars), -matched_patient_id), target="label", positive=1)

test_lr <- makeClassifTask(data=data_[[2]] %>% select(one_of(selected_vars), -matched_patient_id), target="label", positive=1)

lr_lrn <- makeLearner(cl = "classif.logreg", predict.type = "prob")
lr_model <- train(learner = lr_lrn, task = train_lr)


pred <- predict(object = model, task = test_mlr)
pr1 <- pr.curve(scores.class0 = pred$data$prob.1, 
                weights.class0 = (as.numeric(pred$data$truth) - 1), 
                curve = TRUE)
plot(pr1)
write_csv(as.data.frame(pr1$curve), 'xgboost.csv')

```