---
title: "BI data exploration"
author: "Norman Poh"
date: "6 September 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 2017-09-06 -------------------------------------------------------
Misc code involved in data exploration

## List variables that should not have been included
```{r}
config$Column[is_included == FALSE]
```

## Analyse variables that should not have been included before deleting them -- I want to know why these variables are not good

```{r}
config$Column[is_included == FALSE]

ggplot(dat, aes(SYMP_CNT, group=label, fill=label)) + 
     geom_histogram(binwidth=0.5,  alpha = .2)

ggplot(dat, aes(SYMP_CNT, group=label, fill=label)) + 
  geom_density(alpha=.5) 

# Examine the lookback days
ggplot(dat, aes(LOOKBACK_DAYS, group=label, fill=label)) + 
  geom_density(alpha=.5) 

dat$int_O_PULM_F_T_6_M_WALK
ggplot(dat, aes(int_O_PULM_F_T_6_M_WALK, group=label, fill=label)) + 
  geom_density(alpha=.5) 

dat_<-subset(dat, int_O_PULM_F_T_6_M_WALK !=0)
ggplot(dat_, aes(int_O_PULM_F_T_6_M_WALK, group=label, fill=label)) + geom_density(alpha=.5) 

```

## 2017-09-07 -------------------------------------------------------
Data analysis

## ml-R complained because data set contains NA so we need to remove them
```{r}
  
matches <- as.factor( data_[[1]]$matched_patient_id )
if (sum(is.na(data_[[1]]$matched_patient_id)) == 0) {
  cat("There is no NA in the matched patient id, which is good.\n")
} else {
  data_[[1]]<- data_[[1]] %>% filter(!is.na(matched_patient_id))
  matches <- as.factor(dat2$matched_patient_id)
  sum(is.na(dat2$matched_patient_id))
}
```
## Prepare xgboost as is

```{r}
dat2 <- data_[[1]] %>% select(-one_of("matched_patient_id"))

ids <- utils_get_ids(dat2, config)
df <- utils_get_variables(dat2, config)

# There should be only patient_id (the key)
setdiff(colnames(data_[[1]]), colnames(df))

# Setup dataset - for imputing missing values have a look at the vignette
target = "label"

#dat2 <- dat %>% select(-one_of("label"))

dataset <- makeClassifTask(id="BC", data=df, 
                           target=target,
                           positive=1, 
                           blocking=matches)

# make learner
lrn_xgb <- makeLearner(cl = "classif.xgboost", predict.type = "prob")

# make resample object with a three-fold corss validation
rdesc <- makeResampleDesc(method = "CV", iters = 3)
```

```{r}
# resample
res <- resample(learner = lrn_xgb, task = dataset, resampling = rdesc)

# make pr curve:
pr_curve <- perf_binned_perf_curve(pred = res$pred)
```
## Try plotting
```{r}
ggplot( data = as_tibble(pr_curve$curve) ) + 
          geom_point( mapping = aes(x = rec_binned, y = prec) ) +
          geom_smooth( mapping = aes(x = rec_binned, y = prec) ) 

```
## OK, we know that xgboost is running correctly. Now, we can run a full-fledged version

## Parameters for xgboost
```{r results = "hide"}

recall_thrs <- 10
random_search_iter <- 50L

output_folder = "xgboost"
utils_create_output_folder(output_folder)
```

##  Setup modelling

```{r}
target_vector <- dat2$label
  
# Define weights as the inverse class frequency
target_vector = getTaskTargets(dataset)
target_tab = as.numeric(table(target_vector))
iw = 1/target_tab[target_vector]

# Define XGboost learner
lrn <- makeLearner("classif.xgboost", predict.type="prob")

lrn$par.vals = list(
  nrounds = 100,
  verbose = F,
  objective = "binary:logistic"
  # to restrict memory and  cpu usage set nthreads = 2
  # for multiclass use objective = "multi:softmax"
)

# Wrap our learner so it will randomly downsample the majority class
lrn <- makeUndersampleWrapper(lrn)

# Define hyper parameters, read this https://goo.gl/CMQxha and XGBoost's docs
ps = makeParamSet(
  makeNumericParam("eta", lower=0.01, upper=0.3),
  makeIntegerParam("max_depth", lower=2, upper=6),
  makeIntegerParam("min_child_weight", lower=1, upper=5),
  makeNumericParam("colsample_bytree", lower=.5, upper=1),
  makeNumericParam("subsample", lower=.5, upper=1),
  makeNumericParam("usw.rate", lower=.5, upper=1)
)

# Define random search
ctrl <- makeTuneControlRandom(maxit=random_search_iter, tune.threshold=F)

# Define performane metrics - use at least 2, otherwise get_results won't work
pr10 <- perf_make_pr_measure(recall_thrs, "pr10")
m2 <- auc
m3 <- setAggregation(pr10, test.sd)
m4 <- setAggregation(auc, test.sd)
# It's always the first in the list that's used to rank hyperparams in tuning
m_all <- list(pr10, m2, m3, m4)

# Define outer and inner resampling strategies
outer <- makeResampleDesc("CV", iters=3, stratify=T, predict = "both")
inner <- makeResampleDesc("CV", iters=3, stratify=T)

matching <- TRUE
if (matching){
  outer$stratify <- FALSE
  inner$stratify <- FALSE
}

# Define wrapped learner: this is mlR's way of doing nested CV on a learner
lrn_wrap <- makeTuneWrapper(lrn, resampling=inner, par.set=ps, control=ctrl,
                            show.info=F, measures=m_all)
```

##  Training model with nested CV and save results

```{r}
parallelStartSocket(detectCores(), level="mlr.tuneParams")
res <- resample(lrn_wrap, dataset, resampling=outer, models=T, weights=iw,
                extract=getTuneResult, show.info=F, measures=m_all)
parallelStop()
readr::write_rds(res, file.path(output_folder, "all_results.rds"))

```


## Train the xgboost -- can take a while

```{r}
# train model
xgb_model <- train(learner = lrn_xgb,
                   task = dataset)

# make predictions
pred <- predict(object = xgb_model, newdata = cars_num)

pred$threshold

```